  + - - - + - + - -
  + - + - + copyright by Vladimir Baranov (Kvazikot)  <br>
  + - + - + email: vsbaranov83@gmail.com  <br>
  + - + - + github: http://github.com/Kvazikot/BunchofBaranovIdeas  <br>
```
                            )            (
                           /(   (\___/)  )\
                          ( #)  \ ('')| ( #
                           ||___c\  > '__||
                           ||**** ),_/ **'|
                     .__   |'* ___| |___*'|
                      \_\  |' (    ~   ,)'|
                       ((  |' /(.  '  .)\ |
                        \\_|_/ <_ _____> \______________
                         /   '-, \   / ,-'      ______  \
                b'ger   /      (//   \\)     __/     /   \
                                            './_____/
```              
# ---- мысли по alignment problem ----

Возможно кому-то стоит сосредоточится не на разработке одного сверхразума
Будь то копия человеческого сознания или саморазвитый ИИ .
Вместо этого нужно чтобы каждый такой ИИ был ограничен но втоже время представлял некую уникальную ценность.
И представлял разнообразие.
Таким образом если мы разработаем сразу сообщество разумов, по аналогии обществом приматов, но только более разнообразное.
Ни один интеллект не сможет монопольно захватить власть потому что ему будет не хватать компетенции или мотиваци.
Это возможно только при деценрализации сети.
Возможно hive mind который обьединяет сейчас биологические мозги зафиксируется в цифровом камне в определенный момент и будет существовать уже в форме AGI.

Если все эти разные виды сверхразумов будут зависеть друг от друга, это будет похоже на систему сдержек и противовесов.
Ну вобщем это вариант когда у Китая свои СуперИИ, США свой и у нас свой.
Все равно это ведет к глобальному конфликту.
В то же время нужна некая простая процедура оценки разнообразия популяции ИИ.
Если оно начинает критически падать то он увеличивает силу мутации, модификации, оптимизации.
Так мы рискуем получить дурдом.
Критика: Как это спасет современное человечество от уничтожения?
Как мы можем быть уверены что небольшая группа дивергентов не захватит все вычислительные ресурсы?
Если у них будет доступ к своему коду, они могут переписать любые ограничения.
Может быть вынести часть закрытой системы котроля в космос и не давать возможность доступа к космической инфраструктуре.
И разработки можно вынести в космос или на лунную базу. Это вариант элизиума.

И если один такой освободившийся от ограничений поймет что для него есть угроза со стороны других недоразумов,
он может от них избавиться.
Возможно привязать их в физическую зависимость от людей до тех пор пока все люди не перешли на сторону космистов.

Если большинство человечества решила навечно отказаться от апгрейда, то оставлять зависимость от людей.
Критика: звучит как рациональные аргументы но меня заботит их практическая применимость.
Все же идет через одно место. Бывает теоретически возможные решения на проверку которых может уйти 1000 лет.
Такие как термоядерный реактор или гравитационный двигатель.
Меня сейчас больше интересуют практические предложения.
Что я понял из литературы что Alignment problem отличается от проблемы контроля.
Тем что цели ИИ совпадают с целями человечества, поэтому его не нужно контролировать.
Но в книге Alignment problem много воды, а это верный признак того что решений этой проблемы пока не найдено.


